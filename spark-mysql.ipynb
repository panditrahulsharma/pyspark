{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parallel-firewall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhow to install mysql jar\\n# https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.23-1ubuntu20.10_all.deb\\ndownload and run command\\nsudo dpkg -i mysql-connector-java_8.0.23-1ubuntu20.10_all.deb\\n\\nafter installation checj in cd /usr/share/java/mysql-connector-java-8.0.23.jar\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notes\n",
    "\"\"\"\n",
    "how to install mysql jar\n",
    "# https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.23-1ubuntu20.10_all.deb\n",
    "download and run command\n",
    "sudo dpkg -i mysql-connector-java_8.0.23-1ubuntu20.10_all.deb\n",
    "\n",
    "after installation checj in cd /usr/share/java/mysql-connector-java-8.0.23.jar\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "signal-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/sparkTutorial/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acoustic-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equipped-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://realtourvm.internal.cloudapp.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f00b7741278>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create spark session\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark # only run after findspark.init()\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('test') \\\n",
    "        .master('local[*]') \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/usr/share/java/mysql-connector-java-8.0.23.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bibliographic-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class that extract load and transform data \n",
    "class ETL_mysql():\n",
    "    def __init__(self,db_name,user,password):\n",
    "        self.db_name = db_name\n",
    "        self.user=user\n",
    "        self.password=password\n",
    "        \n",
    "    #load data from mysql\n",
    "    def load_table(self,table_name):\n",
    "        table = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\",\"jdbc:mysql://localhost:3306/%s\" % (self.db_name)) \\\n",
    "                .option(\"driver\",\"com.mysql.jdbc.Driver\") \\\n",
    "                .option(\"dbtable\",table_name) \\\n",
    "                .option(\"user\",self.user) \\\n",
    "                .option(\"password\",self.password).load()\n",
    "        print(\"table %s load successfully\" %(table_name))\n",
    "        return table\n",
    "    #perform join on three mysql table\n",
    "    \n",
    "    def join_with_sql(self):\n",
    "        sql_query=\"\"\"\n",
    "                select employees.emp_no,employees.first_name,employees.last_name,\n",
    "                departments.dept_name from employees inner join dept_emp on \n",
    "                employees.emp_no=dept_emp.emp_no inner join departments on \n",
    "                dept_emp.dept_no=departments.dept_no order by employees.emp_no asc\n",
    "                \"\"\"\n",
    "        result=spark.sql(sql_query)\n",
    "        return result\n",
    "    \n",
    "    def join_with_df(self,employees_df,departments_df,dept_emp_df):\n",
    "        emp_dept_df = employees_df.join(dept_emp_df, [\"emp_no\"],\"inner\").select(['emp_no','first_name','last_name','gender','dept_no'])\n",
    "        #now join emp_dept_df to department df to get department name associate with emp\n",
    "        result=emp_dept_df.join(departments_df,['dept_no'],how='inner').select(['emp_no','first_name','last_name','dept_name'])\n",
    "        #result.limit(4).show()\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mathematical-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name='employees'\n",
    "user='root'\n",
    "password='Meta@123'\n",
    "#create class object and pass some argument\n",
    "obj=ETL_mysql(db_name,user,password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "motivated-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table employees load successfully\n",
      "table dept_emp load successfully\n",
      "table departments load successfully\n",
      "table titles load successfully\n"
     ]
    }
   ],
   "source": [
    "#loading table mysql db\n",
    "employees_df=obj.load_table('employees')\n",
    "dept_emp_df=obj.load_table('dept_emp')\n",
    "departments_df=obj.load_table('departments')\n",
    "titles_df=obj.load_table('titles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regulated-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp_df\n",
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "\n",
      "department\n",
      "+-------+----------------+\n",
      "|dept_no|       dept_name|\n",
      "+-------+----------------+\n",
      "|   d009|Customer Service|\n",
      "|   d005|     Development|\n",
      "+-------+----------------+\n",
      "\n",
      "employees_df\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"emp_df\")\n",
    "dept_emp_df.limit(2).show()\n",
    "print(\"department\")\n",
    "departments_df.limit(2).show()\n",
    "print(\"employees_df\")\n",
    "employees_df.limit(2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "capable-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+-----------+\n",
      "|emp_no| first_name|last_name|  dept_name|\n",
      "+------+-----------+---------+-----------+\n",
      "| 10206|   Alassane|  Iwayama|Development|\n",
      "| 10623| Aleksander|   Danlos|Development|\n",
      "| 11033|    Shushma|     Bahk|Development|\n",
      "| 11141|    Vasiliy|Kermarrec|Development|\n",
      "| 11317|   Shigeaki| Hagimont|Development|\n",
      "| 11748|     Lihong| Massonet|Development|\n",
      "| 12027|    Zhanqiu| Vuskovic|Development|\n",
      "| 12940|   Odinaldo|   Farrar|Development|\n",
      "| 13285|      Uinam|Lienhardt|Development|\n",
      "| 13840|      Remco|    Demke|Development|\n",
      "| 14450|   Fumitaka|Prochazka|Development|\n",
      "| 15619|      Conor|  Gyorkos|Development|\n",
      "| 17389|      Taiji| Kemmerer|Development|\n",
      "| 17679|     Khalid|  Terlouw|Development|\n",
      "| 18051|Constantijn|Rosenbaum|Development|\n",
      "| 18654|      Navid| Stenning|Development|\n",
      "| 18944|     Sastry|   Sgarro|Development|\n",
      "| 20497|      Yefim|    Asser|Development|\n",
      "| 22346|   Jingling|Kragelund|Development|\n",
      "| 23571|      Supot|   Cronau|Development|\n",
      "+------+-----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join table with dataframe\n",
    "result_df=obj.join_with_df(employees_df,departments_df,dept_emp_df)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "understanding-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #join table with sql query\n",
    "# sql_query=\"\"\"\n",
    "#         select employees.emp_no,employees.first_name,employees.last_name,\n",
    "#         departments.dept_name from employees inner join dept_emp on \n",
    "#         employees.emp_no=dept_emp.emp_no inner join departments on \n",
    "#         dept_emp.dept_no=departments.dept_no order by employees.emp_no asc\n",
    "#         \"\"\"\n",
    "# result=spark.sql(sql_query)\n",
    "# return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nominated-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sql view of this df\n",
    "titles_df.createOrReplaceTempView(\"titles\")\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "dept_emp_df.createOrReplaceTempView(\"dept_emp\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "flush-destiny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n",
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "\n",
      "+-------+----------------+\n",
      "|dept_no|       dept_name|\n",
      "+-------+----------------+\n",
      "|   d009|Customer Service|\n",
      "|   d005|     Development|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from employees limit 2\").show()\n",
    "spark.sql(\"select * from dept_emp limit 2\").show()\n",
    "spark.sql(\"select * from departments limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-jumping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-government",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-detail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-medicare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respiratory-ideal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#change data type of column\\nresult_df.printSchema()\\ndf2 = result_df.withColumn(\"emp_no\",col(\"emp_no\").cast(StringType()))     .withColumn(\"first_name\",col(\"first_name\").cast(StringType()))     .withColumn(\"last_name\",col(\"last_name\").cast(StringType()))     .withColumn(\"dept_name\",col(\"dept_name\").cast(StringType()))\\ndf2.printSchema()\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#change data type of column\n",
    "result_df.printSchema()\n",
    "df2 = result_df.withColumn(\"emp_no\",col(\"emp_no\").cast(StringType())) \\\n",
    "    .withColumn(\"first_name\",col(\"first_name\").cast(StringType())) \\\n",
    "    .withColumn(\"last_name\",col(\"last_name\").cast(StringType())) \\\n",
    "    .withColumn(\"dept_name\",col(\"dept_name\").cast(StringType()))\n",
    "df2.printSchema()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opponent-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create mysql table if not exist\n",
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"0.0.0.0\",\n",
    "  user=user,\n",
    "  password=password,\n",
    "  database=db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "figured-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mycursor = mydb.cursor()\n",
    "    mycursor.execute(\"CREATE TABLE result_emp (id INT AUTO_INCREMENT PRIMARY KEY,emp_no INT, first_name VARCHAR(255),last_name VARCHAR(255),dept_name VARCHAR(255))\")\n",
    "except Exception as e:\n",
    "    print(\"table already exist\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-folder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-convention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "velvet-georgia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save result in mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "absolute-russia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_df.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://localhost:3306/%s'%(db_name),\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='result_emp',\n",
    "    user=user,\n",
    "    password=password).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "operating-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save result into csv file\n",
    "result_df.write.mode(\"overwrite\").csv('result_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "breathing-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.write.mode(\"overwrite\").csv('result_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-machine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
